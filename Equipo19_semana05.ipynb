{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH2xcBtAiezwylFpSxu7Ob",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a01685424/CloudComputing_TC4031/blob/main/Equipo19_semana05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Maestría en Inteligencia Artificial Aplicada**\n",
        "##**Curso: Procesamiento de Lenguaje Natural (NLP)**\n",
        "###Tecnológico de Monterrey\n",
        "###Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "## **Adtividad Semanas 05**\n",
        "\n",
        "###**5.2 Actividad: Continuando con caso de estudio Amazon-Yelp-Imdb**\n",
        "\n",
        "#**Matricula-Alumnos:** \n",
        "\n",
        "**A01793547.- Eddie Guadalupe Elorza Ruiz**\n",
        "\n",
        "**A01685424.-Fernando Alfredo Rojas Estrella**\n",
        "\n",
        "**A00174646.-José Ramiro Adán Charles**\n",
        "\n",
        "**A01793704.-Marco Antonio Vázquez Morales**\n",
        "\n",
        "**A01793131.- Rubén Marcos Ramos Guerrero**"
      ],
      "metadata": {
        "id": "m32FOTlSU6D_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la actividad de esta semana trabajarás en equipos con el modelo de vectores continuos/embebidos FastText,\n",
        "es decir, el modelo desarrollado por Facebook en 2016.\n",
        "Una manera de trabajar con estos modelos pre-entrenados, es generando el vocabulario a partir de tu conjunto\n",
        "de datos de entrenamiento. Para cada palabra de tu vocabulario, podrás sustituirlo por su correspondiente\n",
        "vector continuo. En caso de que no exista el vector para una palabra en particular, se puede eliminar dicha\n",
        "palabra, o bien sustituirla por el vector continuo más cercano. En esta actividad deberás aplicar esta segunda\n",
        "opción. Existen diversas propuestas para utilizar dichos vectores continuos como entrada para modelos de\n",
        "aprendizaje automático. En particular, en esta actividad cada enunciado será sustituido por el vector promedio\n",
        "de todos los tokens que lo forman."
      ],
      "metadata": {
        "id": "vROuUPQXX2D6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 1\n",
        "\n",
        "Descarga los 3 archivos de Canvas. En particular, el archivo de datos de IMDb ya no requiere\n",
        "transformarse para obtener sus 1000 registros. Al cargar los datos de los tres archivos deberás\n",
        "tener un DataFrame de Pandas de 3000 registros, con sus etiquetas. Los archivos los encuentras en\n",
        "Canvas y se llaman: amazon5.txt, imdb5.txt, yelp5.txt."
      ],
      "metadata": {
        "id": "Vk0XzW_QX8SI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando liberías\n",
        "\n",
        "import pandas as pd  \n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer     # Para implementar lematización"
      ],
      "metadata": {
        "id": "_h7-QRj2YFza"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')    # Descarga de stopwords\n",
        "nltk.download('wordnet')      # Descarga para lematización\n",
        "\n",
        "#nltk.download('punkt')        # Descarga tokenizador que ayuda a dividr el texto en enunciados mediante un modelo no-supervisado.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc03e5nH1LGO",
        "outputId": "ef533d72-4480-463d-9831-bbdee6bb0d19"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga de los archivos\n",
        "\n",
        "dfa = pd.read_csv('amazon5.txt', sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
        "dfi = pd.read_csv('imdb5.txt', delimiter='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
        "dfy = pd.read_csv('yelp5.txt', sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
        "\n",
        "\n",
        "print('Total de registros de Amazon:',dfa.shape)\n",
        "print('Total de registros de IMBD:',dfi.shape)\n",
        "print('Total de registros de Yelp:',dfy.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i7_nQ7Hby77",
        "outputId": "3c32690e-e764-40e2-8ae5-9ef1a35e6d95"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de registros de Amazon: (1000, 2)\n",
            "Total de registros de IMBD: (1000, 2)\n",
            "Total de registros de Yelp: (1000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Revisando sets\n",
        "print('\\n- - - - - - - - - - - - - - - - - - - - - - -\\ndfa')\n",
        "dfa.info()\n",
        "print('\\n- - - - - - - - - - - - - - - - - - - - - - -\\ndfi')\n",
        "dfi.info()\n",
        "print('\\n- - - - - - - - - - - - - - - - - - - - - - -\\ndfy')\n",
        "dfy.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HPXfA83mt4Z",
        "outputId": "d4733313-180b-44b2-9a9a-22149bbeb568"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- - - - - - - - - - - - - - - - - - - - - - -\n",
            "dfa\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   review  1000 non-null   object\n",
            " 1   label   1000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 15.8+ KB\n",
            "\n",
            "- - - - - - - - - - - - - - - - - - - - - - -\n",
            "dfi\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   review  1000 non-null   object \n",
            " 1   label   0 non-null      float64\n",
            "dtypes: float64(1), object(1)\n",
            "memory usage: 15.8+ KB\n",
            "\n",
            "- - - - - - - - - - - - - - - - - - - - - - -\n",
            "dfy\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   review  1000 non-null   object\n",
            " 1   label   1000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 15.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dfi tienen un problema ya que la columna label está vacía\n",
        "dfii= dfi.copy()\n",
        "i= 0\n",
        "for ind, fila in dfii.iterrows():\n",
        "  string= dfii.iloc[i,0]\n",
        "  label= re.findall(r'(\\d)$', string)\n",
        "  dfii.iloc[i,1]= int(label[0])\n",
        "  #print(dfii.iloc[1,1])\n",
        "  i+=1\n",
        "# Se obtienen los valores de review de cada fila en label\n",
        "dfii['label'].value_counts()   # Recordemos que debemos tener un 50% de comentarios positivos y de negativos."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SC_z6EqnhDI",
        "outputId": "5ad9ad95-ab6c-423d-81c5-9b239b9f407a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    500\n",
              "1.0    500\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se concatenan los tres conjuntos\n",
        "\n",
        "df = pd.concat([dfa, dfii, dfy], ignore_index=True)   # Se utiliza el \"ignore_index\", para no reinciar cada vez los índices\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P320s8TkcWuD",
        "outputId": "e5f0b4d3-2d6f-4513-d1b1-70224b34dd48"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   review  3000 non-null   object \n",
            " 1   label   3000 non-null   float64\n",
            "dtypes: float64(1), object(1)\n",
            "memory usage: 47.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'].value_counts()   # Recordemos que debemos tener un 50% de comentarios positivos y de negativos."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axfmw6qrYF-k",
        "outputId": "ddb78bf7-b8c9-41cb-8be4-1d4e05a8cde8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    1500\n",
              "1.0    1500\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ocdwKHCezlke",
        "outputId": "b2400c3c-b9ec-4dcf-be67-54051a0b75dc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  label\n",
              "0  So there is no way for me to plug it in here i...    0.0\n",
              "1                        Good case, Excellent value.    1.0\n",
              "2                             Great for the jawbone.    1.0\n",
              "3  Tied to charger for conversations lasting more...    0.0\n",
              "4                                  The mic is great.    1.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3db54d22-ca8a-4309-9f7a-4adfb2b12c8c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So there is no way for me to plug it in here i...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good case, Excellent value.</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great for the jawbone.</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tied to charger for conversations lasting more...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The mic is great.</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3db54d22-ca8a-4309-9f7a-4adfb2b12c8c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3db54d22-ca8a-4309-9f7a-4adfb2b12c8c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3db54d22-ca8a-4309-9f7a-4adfb2b12c8c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 2\n",
        "\n",
        "Realiza de nuevo un proceso de limpieza. Aplica el preprocesamiento que consideres adecuado, sin embargo, deberás aplicar necesariamente alguna de las técnicas de lematización. Como aplicaremos modelos embebidos pre-entrenados, queremos palabras lo más cercanas a las existentes en un idioma, inglés en este caso. Aplica y justifica cualquier otro proceso de limpieza que consideres adecuado. Recuerda que en esta actividad se usarán vectores embebidos para un problema de clasificación, por lo que deberás tomar de acuerdo a este contexto. Justifica todas las transformaciones que se apliquen."
      ],
      "metadata": {
        "id": "DLaDyWrrYGRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificamos la existencia de los comentarios númericos que habíamos encontrado en tareas anteriores\n",
        "\n",
        "print(df.iloc[1125,:])\n",
        "print(df.iloc[1788,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFtQmclvYG2i",
        "outputId": "5c63bab7-1dfc-4ec1-8d41-ebf05821869b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "review    10/10    1\n",
            "label            1.0\n",
            "Name: 1125, dtype: object\n",
            "review    10/10    1\n",
            "label            1.0\n",
            "Name: 1788, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se realiza la adecuación del texto\n",
        "\n",
        "df.iloc[1125,0]= 'Excellent movie.'\n",
        "df.iloc[1788,0]= 'Excellent movie.'\n",
        "\n",
        "print(df.iloc[1125,:])\n",
        "print(df.iloc[1788,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPh-1O3deS1Z",
        "outputId": "27fc6e21-5bae-4cb9-b363-1dbf750da61e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "review    Excellent movie.\n",
            "label                  1.0\n",
            "Name: 1125, dtype: object\n",
            "review    Excellent movie.\n",
            "label                  1.0\n",
            "Name: 1788, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tok(doc):\n",
        "  ##############################################################################\n",
        "  # 1 Simplificar todo a minúsculas\n",
        "  cadena= re.sub(r\"'\", \"\", doc.lower())             # Aseguramos que las negaciones en palabras no se borren.\n",
        "  #print('cadena= ', cadena)\n",
        "  # 2 Considerando solamente tokens de longitud mayor a 1\n",
        "  cadena= re.sub(r'(\\W\\w{1}\\W)', ' ', cadena)\n",
        "  #print('CADENA=', cadena)\n",
        "  # 3 Solo considerar caracteres alfabéticos\n",
        "  cadena= re.sub(r'[^a-zA-Z]', ' ', cadena)         # Consideramos caracteres letras solamente.\n",
        "  cadena= re.sub(r'\\s{2,}', ' ', cadena.strip())\n",
        "  # 4 Tokenizado por palabra\n",
        "  lista= re.split(r'\\s', cadena)\n",
        "  # 5 Eliminando stopwords\n",
        "  negwords= [ 'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'don', \"don't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "  mystopwords= [word for word in stopwords.words('english') if word not in negwords]\n",
        "  tokenes= [word for word in lista if word not in mystopwords]\n",
        "  # Aplicando lematización\n",
        "  wnl = WordNetLemmatizer()\n",
        "  tokens= [ wnl.lemmatize(token) for token in tokenes]\n",
        "  # CODIGO\n",
        "  ##############################################################################\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "7oZwL_0eeSpR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separando la información para realizar su limpieza\n",
        " #     La \"X\" Datos de comentarios.\n",
        " #     La \"Y\" Variable de la evaluación.\n",
        "\n",
        "X = df.review     # Serie de strings\n",
        "Y = df.label      # Serie de enteros 0s y 1s\n",
        "\n",
        "assert X.shape == (3000,)           # verificando que tenemos la dimensiones esperadas.\n",
        "assert Y.shape == (3000,)"
      ],
      "metadata": {
        "id": "8sbuw7CX0c__"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicamos el proceso de limpieza y tokenización:\n",
        "print(X[0:5])\n",
        "Xcleantok = [clean_tok(x) for x in X]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoHBFtZfzI0q",
        "outputId": "d576c88a-7548-427d-af93-a9312299164e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    So there is no way for me to plug it in here i...\n",
            "1                          Good case, Excellent value.\n",
            "2                               Great for the jawbone.\n",
            "3    Tied to charger for conversations lasting more...\n",
            "4                                    The mic is great.\n",
            "Name: review, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Primer ejercicio de limpieza\n",
        "\n",
        "for x in Xcleantok[0:5]:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FFhTPDdzIsn",
        "outputId": "b35c697c-4037-45e5-e166-cf44a70b07d9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['no', 'way', 'plug', 'u', 'unless', 'go', 'converter']\n",
            "['good', 'case', 'excellent', 'value']\n",
            "['great', 'jawbone']\n",
            "['tied', 'charger', 'conversation', 'lasting', 'minute', 'major', 'problem']\n",
            "['mic', 'great']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ibsOxF8Qtoxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8A5aMDmBtopx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yqxiR95gYHAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 3\n",
        "\n",
        "Llamar Xclean a los comentarios procesados y Y a las etiquetas. Realicemos una partición aleatoria\n",
        "con los mismos porcentajes de la práctica pasada para poder comparar dichos resultados con los de\n",
        "esta actividad, a saber, 70%, 15% y 15%, para entrenamiento, validación y prueba,\n",
        "respectivamente. Verifica que obtienes 2100 registros de entrenamiento y 450 para cada uno de\n",
        "validación y prueba."
      ],
      "metadata": {
        "id": "wAuiYNXUYHSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Partición de la información\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xclean= Xcleantok.copy()\n",
        "Y\n",
        "\n",
        "Xtrain, Xtest_Xval, Ytrain, Ytest_Yval= train_test_split(Xclean, Y, test_size=0.3, random_state=42)\n",
        "Xval, Xtest, Yval, Ytest= train_test_split(Xtest_Xval, Ytest_Yval, test_size=0.5, random_state=42)\n",
        "\n",
        "print('Tamaño entrenamiento=', len(Xtrain))\n",
        "print('Tamaño validación=', len(XVal))\n",
        "print('Tamaño prueba=', len(Xtest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snCtQKhQYHpQ",
        "outputId": "39d74bc1-0b56-4cef-afc6-20042ba6bc40"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño entrenamiento= 2100\n",
            "Tamaño validación= 450\n",
            "Tamaño prueba= 450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ngWKbut3YH01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 4\n",
        "\n",
        "Usando el conjunto de entrenamiento genera un vocabulario que no sea mayor a 1500 palabras, ni\n",
        "menor a 1000. ¿Por qué es importante acotar un vocabulario inferior y superiormente? ¿Por qué\n",
        "debe usarse solamente el conjunto de entrenamiento para generar el diccionario? Con este\n",
        "vocabulario que obtienes, filtra los conjuntos de entrenamiento, validación y prueba, de esta\n",
        "manera todos los comentarios usarán solamente palabras válidas de acuerdo a este vocabulario.\n",
        "Indica el tamaño del vocabulario obtenido.\n",
        "Hasta este punto básicamente has realizado transformaciones muy análogas a las de la semana\n",
        "pasada y que son válidas para muchos de los procesos dentro del análisis de textos. En dado caso\n",
        "comenta con tus compañeros de equipo qué diferencias has observado. Veamos ahora la diferencia\n",
        "con respecto a las matrices Tf-idf que aplicaste la semana pasada, con respecto a los vectores preentrenados\n",
        "embebidos."
      ],
      "metadata": {
        "id": "iGS6-sVQYIHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generando vocabulario de entrenamiento\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "frec= Counter()\n",
        "for listas in Xtrain:\n",
        "  frec.update(listas)\n",
        "\n",
        "frecuencia= frec.most_common()\n",
        "vocabulario= list(frec.keys())\n",
        "#print('Vocabulario= ', Vocabulario)\n",
        "print('Tamaño vocabulario de entrenamiento= ', len(vocabulario))\n",
        "\n",
        "# Se quedan las palabras con mayor frecuencia\n",
        "vocatrain= []\n",
        "for palabra, frec in frecuencia:\n",
        "  if len(vocatrain) < 1490:\n",
        "    if frec > 1:\n",
        "      vocatrain.append(palabra)\n",
        "    #print(f'{palabra}: {frec}')\n",
        "\n",
        "print('\\nTamaño vocabulario acortado= ', len(vocatrain))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTZOGa48YIe0",
        "outputId": "2ca0b054-b7ec-43cb-8ca2-2528d531662b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño vocabulario de entrenamiento=  3722\n",
            "\n",
            "Tamaño vocabulario acortado=  1490\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preguntas\n",
        "\n",
        "- ¿Por qué es importante acotar un vocabulario inferior y superiormente?\n",
        "  - **Respuesta:** Una de las principales razones es para evitar el sobre ajuste de los datos, para evitar palabras tan poco comunes o frecuentes que hagan que el modelo se comporte sobre entrenado. Acotar el vocabulario también ayudará a generalizar el modelo representando de mejor manera las características generales de los comentarios que se están analizando.\n",
        "\n",
        "- ¿Por qué debe usarse solamente el conjunto de entrenamiento para generar el diccionario?\n",
        "  - **Respuesta:** Una de las razones principales es por qué al limitar el entrenamiento del modelo al conjunto de entrenamiento se efita la filtración de información, permitiendo la obtención de escenarios más parecidos a la realidad a la que se enfrentará el modelo cuando logre generalizar de forma adecuada."
      ],
      "metadata": {
        "id": "1JqHECKLooao"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-yxw46IqoLt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e8DCwhhAYIn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 5\n",
        "\n",
        "Utilizarás los vectores embebidos FastText preentrenados por Facebook.\n",
        "a. Incluye una tabla comparativa de pros y contras entre los modelos FastText, word2vec de\n",
        "Google y Glove de Stanford. Puedes consultar sus páginas correspondientes:\n",
        "https://fasttext.cc/\n",
        "https://code.google.com/archive/p/word2vec/\n",
        "https://nlp.stanford.edu/projects/glove/"
      ],
      "metadata": {
        "id": "wKTerua7YLlA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dR7FnBIvYL3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Um4xS3IYL8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 6\n",
        "\n",
        "Utiliza el modelo FastText de vectores embebidos pre-entrenados de dimensión 300 para generar\n",
        "un nuevo diccionario clave-valor, donde la “clave” será cada token o palabra de tu vocabulario y el\n",
        "“valor” será su vector embebido de dimensión 300. Este diccionario deberá ser del mismo tamaño\n",
        "que el vocabulario previo que hayas construido previamente.\n",
        "https://fasttext.cc/docs/en/crawl-vectors.html\n",
        "NOTA: Debido a la cantidad de recursos computacionales que demanda cargar los vectores\n",
        "FastText (son 2 millones de vectores), es recomendable que una vez que generes el nuevo\n",
        "vocabulario de vectores embebidos, guardes dicho diccionario en un archivo (pickle, npz o el que\n",
        "consideres más adecuado). Una vez realizado lo anterior, puedes borrar la variable de FastText\n",
        "para liberar memoria RAM. De esta manera, ya tienes tu vocabulario de vectores embebidos de\n",
        "acuerdo a los tokens que consideras más adecuados para tu problema y puedes usarlo rápidamente\n",
        "cuando lo necesites. En dado caso apóyense entre los miembros del equipo de tener dificultades\n",
        "para generar el vocabulario y por mientras puedes usar el archivo del vocabulario que alguno haya\n",
        "generado"
      ],
      "metadata": {
        "id": "-W3pHunoYMJd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dgYITKIdYMVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wZxN14GoYMa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 7\n",
        "\n",
        "Una manera de utilizar los vectores embebidos con modelos de aprendizaje automático en\n",
        "documentos de texto, es asignar a cada comentario filtrado el vector embebido de dimensión 300\n",
        "que resulta de promediar todos sus tokens. Así, en este ejercicio deberás generar los arreglos\n",
        "correspondientes para los conjuntos de entrenamiento, validación y prueba. Los llamaremos\n",
        "trainEmb, valEmb y testEmb, respectivamente. ¿Cuáles son sus dimensiones? ¿Se podrían usar para\n",
        "su representación matrices dispersas (sparse matrices) como en el caso de la matriz Tf-idf?\n",
        "Responde a dichas preguntas."
      ],
      "metadata": {
        "id": "f3zohasyYMrx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jleBsDERYM4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AxRa5Zi6YM9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 8\n",
        "\n",
        "Utiliza los modelos de regresión logística y bosque aleatorio (random forest) y encuentra sus desempeños (accuracy). Compara los resultados con los de la semana anterior."
      ],
      "metadata": {
        "id": "TVVc-LtrYNKy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8D9uFA2IYNYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mmU6HkP8YNdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 9\n",
        "\n",
        "Obtener la matriz de confusión e interpretar sus valores."
      ],
      "metadata": {
        "id": "gNcUOuZAYNsT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R__zu2wCYN9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jDtJv-9SYOCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 10\n",
        "\n",
        "Comenta con tus compañeros de equipo los pasos realizados en esta actividad e incluyan sus\n",
        "conclusiones finales."
      ],
      "metadata": {
        "id": "V6RxjVRsYpnN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uUzbyN9uYrT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EvWgNxoxYrkx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}